<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Playful Words</title>
    <link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="/assets/css/style.css?v=a1ba4a4cc2146c15f967de242850d2b4a364f93c">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <a href="/"><img src="/images/pwlogo-new.png" width="50%"></a>
        <br><br><br>
        <h4><a href="/">Home</a></h4>
        <h4><a href="/research/overview">Research</a></h4>
        <h4><a href="/publications">Publications</a></h4>
        <h4><a href="/team">Team</a></h4>
        <br><br><br>
      </header>
      <section>
        <h2 id="chatterbox">ChatterBox</h2>
<p><em>Eric Chu</em></p>

<p>Speech synthesis in tutor mode. Using phones for literacy learning is an empowering application of mobile technology, but there are elements of the human tutor that have yet to be replicated in current apps. Namely, when reading a story, a tutor is likely to be more expressive and colorful in tone. When encountering a new word, a tutor might emphasize the vowel phoneme or stress a consonant pair the child has yet to master. By modeling speech with deep neural networks, our speech synthesizer will be able to interpolate between speaking styles, switching from ‘normal’ mode to ‘tutor’ mode as needed.</p>


      </section>
      <footer>
        <p>This research project is part of the <a href="https://www.media.mit.edu/groups/social-machines/overview/">Laboratory for Social Machines</a> at the MIT Media Lab</p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>

  
  </body>
</html>